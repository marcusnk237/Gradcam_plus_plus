{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpEqady+crNGUY1fY8tHBD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusnk237/Gradcam_plus_plus/blob/main/Gradcam_plus_plus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "8NenAc___4t2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTx_EJ-A-WTw"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_addons\n",
        "import cv2\n",
        "import gc\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import tensorflow.compat.v1.keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.collections as mcoll\n",
        "import matplotlib as mpl\n",
        "mpl.style.use('seaborn')\n",
        "import numpy as np\n",
        "import itertools\n",
        "import logging\n",
        "# Set random seed\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grad-CAM ++ Function"
      ],
      "metadata": {
        "id": "Q8flk7uh_-Je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Output image : The output will be an image, with the signal as the foreground, and the heatmap as the background"
      ],
      "metadata": {
        "id": "OjMwNyJAC3eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cam_image_output (model, data , layer_name , W , H):\n",
        "        \"\"\"\n",
        "        model: The Deep Learning model\n",
        "        data : A input data\n",
        "        layer_name : The target layer for explanation\n",
        "        W : Width of the result heatmap\n",
        "        H : Height of the result heatmap\n",
        "        \"\"\"\n",
        "        # input layer, model output layer and target layer\n",
        "        grad_model = tf.keras.models.Model(inputs=[model.inputs],outputs=[model.get_layer(layer_name).output, model.output])     \n",
        "        \n",
        "        # Getting gradients of input layer, model output layer (predictions) and target layer\n",
        "        with tf.GradientTape() as tape:\n",
        "            inputs = np.expand_dims(data,axis=0)\n",
        "            conv_outs, predictions = grad_model(inputs) \n",
        "            class_idx = tf.argmax(predictions[0])\n",
        "            y_c = predictions[:, class_idx]\n",
        "\n",
        "        batch_grads = tape.gradient(y_c, conv_outs) \n",
        "        grads = batch_grads[0]\n",
        "        \n",
        "        # First, second and third derivative of output gradient\n",
        "        first = tf.exp(y_c) * grads\n",
        "        second = tf.exp(y_c) * tf.pow(grads, 2)\n",
        "        third = tf.exp(y_c) * tf.pow(grads, 3)\n",
        "        \n",
        "        # Compute salienty maps for the class_idx prediction\n",
        "        global_sum = tf.reduce_sum(tf.reshape(conv_outs[0], shape=(-1, first.shape[1])), axis=0)\n",
        "        alpha_num = second\n",
        "        alpha_denom = second * 2.0 + third * tf.reshape(global_sum, shape=(1,1,first.shape[1]))\n",
        "        alpha_denom = tf.where(alpha_denom != 0.0, alpha_denom, tf.ones(shape=alpha_denom.shape))\n",
        "        alphas = alpha_num / alpha_denom\n",
        "        weights = tf.maximum(first, 0.0)\n",
        "        alpha_normalization_constant = tf.reduce_sum(tf.reduce_sum(alphas, axis=0), axis=0)\n",
        "        alphas /= tf.reshape(alpha_normalization_constant, shape=(1,1,first.shape[1]))\n",
        "        alphas_thresholding = np.where(weights, alphas, 0.0)\n",
        "\n",
        "        alpha_normalization_constant = tf.reduce_sum(tf.reduce_sum(alphas_thresholding, axis=0),axis=0)\n",
        "        alpha_normalization_constant_processed = tf.where(alpha_normalization_constant != 0.0, alpha_normalization_constant,\n",
        "                                                          tf.ones(alpha_normalization_constant.shape))\n",
        "\n",
        "        alphas /= tf.reshape(alpha_normalization_constant_processed, shape=(1,1,first.shape[1]))\n",
        "        deep_linearization_weights = tf.reduce_sum(tf.reshape((weights*alphas), shape=(-1,first.shape[1])), axis=0)\n",
        "        grad_CAM_map = tf.reduce_sum(deep_linearization_weights * conv_outs[0], axis=-1)\n",
        "        \n",
        "        # Normalization\n",
        "        cam = np.maximum(grad_CAM_map, 0)\n",
        "        cam = cam / np.max(cam)  \n",
        "        \n",
        "        # Turn result into a heatmap\n",
        "        heatmap=[]\n",
        "        heatmap.append(cam.tolist())\n",
        "        big_heatmap = cv2.resize(np.array(heatmap), dsize=(W, H),interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # Plotting input data and the Grad-CAM ++ results\n",
        "        plt.imshow(big_heatmap, cmap='rainbow', aspect=\"auto\", interpolation='nearest',extent=[0,len(data)-1,(-1*H),H], alpha=0.9)\n",
        "        plt.title(f\"Grad-CAM ++ Visualization\")\n",
        "        plt.plot(data*80,'k')\n",
        "        plt.grid(False)\n",
        "        plt.colorbar()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "0eLfvGTp-iX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Output signal : The output will be the signal with each important segment draw with a color which represent the importance for the classification/prediction"
      ],
      "metadata": {
        "id": "gj1WvrhpDIgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multicolored_lines(x,y,heatmap,title_name):\n",
        "    fig, ax = plt.subplots()\n",
        "    lc = colorline(x, y, heatmap,cmap='rainbow')\n",
        "    plt.colorbar(lc)\n",
        "    lc.set_linewidth(2)\n",
        "    lc.set_alpha(0.8)\n",
        "    plt.xlim(x.min(), x.max())\n",
        "    plt.ylim(y.min(), y.max())\n",
        "    plt.title(title_name)\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "def colorline(x, y, heatmap,cmap='rainbow'):\n",
        "    z = np.array(heatmap)\n",
        "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "    lc = mcoll.LineCollection(segments, array=z, cmap=cmap)\n",
        "    ax = plt.gca()\n",
        "    ax.add_collection(lc)\n",
        "    return lc\n"
      ],
      "metadata": {
        "id": "2PiRCWKJCNO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cam_1d_output (model, data , layer_name , N):\n",
        "        \"\"\"\n",
        "        model: The Deep Learning model\n",
        "        data : A input data\n",
        "        layer_name : The target layer for explanation\n",
        "        W : Width of the result heatmap\n",
        "        H : Height of the result heatmap\n",
        "        \"\"\"\n",
        "        # input layer, model output layer and target layer\n",
        "        grad_model = tf.keras.models.Model(inputs=[model.inputs],outputs=[model.get_layer(layer_name).output, model.output])     \n",
        "        \n",
        "        # Getting gradients of input layer, model output layer (predictions) and target layer\n",
        "        with tf.GradientTape() as tape:\n",
        "            inputs = np.expand_dims(data,axis=0)\n",
        "            conv_outs, predictions = grad_model(inputs) \n",
        "            class_idx = tf.argmax(predictions[0])\n",
        "            y_c = predictions[:, class_idx]\n",
        "\n",
        "        batch_grads = tape.gradient(y_c, conv_outs) \n",
        "        grads = batch_grads[0]\n",
        "        \n",
        "        # First, second and third derivative of output gradient\n",
        "        first = tf.exp(y_c) * grads\n",
        "        second = tf.exp(y_c) * tf.pow(grads, 2)\n",
        "        third = tf.exp(y_c) * tf.pow(grads, 3)\n",
        "        \n",
        "        # Compute salienty maps for the class_idx prediction\n",
        "        global_sum = tf.reduce_sum(tf.reshape(conv_outs[0], shape=(-1, first.shape[1])), axis=0)\n",
        "        alpha_num = second\n",
        "        alpha_denom = second * 2.0 + third * tf.reshape(global_sum, shape=(1,1,first.shape[1]))\n",
        "        alpha_denom = tf.where(alpha_denom != 0.0, alpha_denom, tf.ones(shape=alpha_denom.shape))\n",
        "        alphas = alpha_num / alpha_denom\n",
        "        weights = tf.maximum(first, 0.0)\n",
        "        alpha_normalization_constant = tf.reduce_sum(tf.reduce_sum(alphas, axis=0), axis=0)\n",
        "        alphas /= tf.reshape(alpha_normalization_constant, shape=(1,1,first.shape[1]))\n",
        "        alphas_thresholding = np.where(weights, alphas, 0.0)\n",
        "\n",
        "        alpha_normalization_constant = tf.reduce_sum(tf.reduce_sum(alphas_thresholding, axis=0),axis=0)\n",
        "        alpha_normalization_constant_processed = tf.where(alpha_normalization_constant != 0.0, alpha_normalization_constant,\n",
        "                                                          tf.ones(alpha_normalization_constant.shape))\n",
        "\n",
        "        alphas /= tf.reshape(alpha_normalization_constant_processed, shape=(1,1,first.shape[1]))\n",
        "        deep_linearization_weights = tf.reduce_sum(tf.reshape((weights*alphas), shape=(-1,first.shape[1])), axis=0)\n",
        "        grad_CAM_map = tf.reduce_sum(deep_linearization_weights * conv_outs[0], axis=-1)\n",
        "        \n",
        "        # Normalization\n",
        "        cam = np.maximum(grad_CAM_map, 0)\n",
        "        cam = cam / np.max(cam)  \n",
        "        \n",
        "        # Turn result into a heatmap\n",
        "        heatmap=[]\n",
        "        heatmap.append(cam.tolist())\n",
        "        big_heatmap = cv2.resize(np.array(heatmap), dsize=(W, H),interpolation=cv2.INTER_CUBIC)\n",
        "        x = np.linspace(0, N, len(data))\n",
        "        multicolored_lines(x,data[:, 0],big_heatmap[0],f\"GradCAM ++ Visualization\")"
      ],
      "metadata": {
        "id": "F2XrfqnMB6C8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}